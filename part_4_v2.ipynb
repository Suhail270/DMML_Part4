{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# loading the dataset\n",
    "x_train = pd.read_csv('CompleteDataset/x_train_all.csv')\n",
    "y_train = pd.read_csv('CompleteDataset/y_train_all.csv')\n",
    "x_test = pd.read_csv('CompleteDataset/x_test_all.csv')\n",
    "y_test = pd.read_csv('CompleteDataset/y_test_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    auc\n",
    ")\n",
    "\n",
    "def get_metrics(clf, x_train, y_train, x_test, y_test):\n",
    "    # transform pandas dataset to a 1d numpy array\n",
    "    y_train = y_train.to_numpy().ravel()\n",
    "    y_test = y_test.to_numpy().ravel()\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    y_pred_probs = clf.predict_proba(x_test)\n",
    "    # finding the accuracy\n",
    "    accuracy = accuracy_score(y_test,y_pred )\n",
    "    # finding the f1\n",
    "    f1 = f1_score(y_pred, y_test, average=\"weighted\")\n",
    "    # finding the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    # finding the precision\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\",zero_division=1)\n",
    "    # finding the recall\n",
    "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    # finding the roc\n",
    "    Y_test_bin = label_binarize(y_test, classes=[0, 1, 2,3,4,5,6,7,8,9])\n",
    "    roc = roc_auc_score(Y_test_bin, y_pred_probs, multi_class=\"ovr\", average=\"weighted\")\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(Y_test_bin.ravel(), y_pred_probs.ravel())\n",
    "    # plotiing the AUC graph\n",
    "    auc_val = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='AUC (area = %0.2f)' % auc_val)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    # plotting the confusion matrix \n",
    "    out=ConfusionMatrixDisplay(conf_matrix,display_y_train=clf.classes_)\n",
    "    out.plot()\n",
    "    plt.show()\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall/Sensitivity/True Positive Rate:\", recall)\n",
    "    print(\"False Positive Rate:\", fpr)\n",
    "    print(\"Area under ROC curve:\", roc)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import(\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    cross_val_predict\n",
    ")\n",
    "\n",
    "def get_cross_val_metrics(clf, x_train, y_train):\n",
    "    # transform pandas dataset to a 1d numpy array\n",
    "    y_train = y_train.to_numpy().ravel()\n",
    "    y_test = y_test.to_numpy().ravel()\n",
    "\n",
    "    # Perform 10-fold cross-validation\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(clf, x_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "    # Print the cross-validation accuracy scores\n",
    "    print(\"Cross-validation accuracy scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    print(\"Accuracy standard deviation:\", cv_scores.std())\n",
    "\n",
    "    # Plotting the cross-validation scores\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(range(len(cv_scores)), cv_scores, color='lightgreen')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Cross-Validation Scores')\n",
    "    plt.ylim(0, 1)  # Set the y-axis limits if needed\n",
    "    plt.show()\n",
    "\n",
    "    # Get predicted y_train for each fold\n",
    "    y_pred = cross_val_predict(clf, x_train, y_train, cv=cv)\n",
    "\n",
    "    # Compute overall precision, recall, F1 score, and support (unused)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_train, y_pred, average='macro')\n",
    "\n",
    "    # Print overall metrics\n",
    "    print(\"Overall Precision:\", precision)\n",
    "    print(\"Overall Recall:\", recall)\n",
    "    print(\"Overall F1 Score:\", f1)\n",
    "\n",
    "    # Initialize variables to store overall metrics and confusion matrix\n",
    "    overall_conf_matrix = np.zeros((len(np.unique(y_train)), len(np.unique(y_train))))\n",
    "\n",
    "    # Loop over each fold\n",
    "    for i, (train_idx, test_idx) in enumerate(cv.split(x_train, y_train)):\n",
    "        X_train, X_test = x_train.iloc[train_idx], x_train.iloc[test_idx]\n",
    "        y_train, y_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "        # Fit the classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_fold_pred = clf.predict(X_test)\n",
    "\n",
    "        # Compute and plot confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_test, y_fold_pred)\n",
    "        overall_conf_matrix += conf_matrix\n",
    "\n",
    "    # Calculate and print average confusion matrix\n",
    "    y_train_annot = list(np.unique(np.array(y_train)))\n",
    "    avg_conf_matrix = overall_conf_matrix / 10\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(avg_conf_matrix, annot=True, fmt=\".2f\", xticky_train=y_train_annot, yticky_train=y_train_annot)\n",
    "    plt.title(\"Average Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted y_train\")\n",
    "    plt.ylabel(\"Actual y_train\")\n",
    "    plt.show()\n",
    "\n",
    "    X = x_train.values  # Convert DataFrame to NumPy array\n",
    "    y = y_train.values  # Convert DataFrame to NumPy array\n",
    "\n",
    "    y_scores = np.zeros((len(y), len(np.unique(y))))\n",
    "\n",
    "    for train, test in cv.split(X, y):\n",
    "        clf.fit(X[train], y[train])\n",
    "        y_scores[test] = clf.predict_proba(X[test])\n",
    "\n",
    "    # Compute overall ROC-AUC score\n",
    "    roc_auc = roc_auc_score(label_binarize(y, classes=np.unique(y)), y_scores, average='macro')\n",
    "\n",
    "    # Print overall ROC-AUC score\n",
    "    print(\"Overall ROC-AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# despite the name, logisticRegression is implemented as a linear model for classsification, as specified in the coursework specs\n",
    "# used \"newton-cholesky\" solver because it converges faster\n",
    "linear_clf = LogisticRegression(random_state=0, solver=\"newton-cholesky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set with 10-fold cross-valiation\n",
    "get_cross_val_metrics(linear_clf, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set without 10-fold cross-valiation\n",
    "get_metrics(linear_clf, x_train, y_train, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing set\n",
    "get_metrics(linear_clf, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# experiment by changing:\n",
    "# act func,\n",
    "# no of layers,\n",
    "# size of layers,\n",
    "# learning rate,\n",
    "# epochs,\n",
    "# momentum,\n",
    "# validation threshold\n",
    "\n",
    "mlp_clf = MLPClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf.fit(x_train, y_train)\n",
    "y_pred = mlp_clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search(clf, x_train, y_train):\n",
    "    # transform pandas dataset to a 1d numpy array\n",
    "    y_train = y_train.to_numpy().ravel()\n",
    "    # y_test = y_test.to_numpy().ravel()\n",
    "\n",
    "    # dictionary of parameters to be varied (keys), and arrays of possible values (values)\n",
    "    param_grid = {\n",
    "        # number of layers and their sizes\n",
    "        \"hidden_layer_sizes\": [(10,30,10),(20,)],\n",
    "        # activation functions\n",
    "        \"activation\": [\"tanh\", \"relu\"],\n",
    "        # solver\n",
    "        \"solver\": [\"sgd\", \"adam\"],\n",
    "        \"alpha\": [0.0001, 0.05],\n",
    "        \"learning_rate\": [\"constant\",\"adaptive\"],\n",
    "        \"n_iter_no_change\": [10],\n",
    "        \"max_iter\": [200],\n",
    "        \"momentum\": [0.9]\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
